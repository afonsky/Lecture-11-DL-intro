# Artificial Neural Networks: Overview

* [ANN](https://en.wikipedia.org/wiki/Artificial_neural_network) is a flxible class of models, which can find highly non-linear relations in I/O
* ANN is an old technology, revived with recent boom in GPU/TPU, novel algorithms, revenue generation and big investors
* ANN is built from neurons, basic building blocks
* **ANN encompasses many infrastructures**
* ANN help focus efforts on engineering infrastructure, rather than engineering input features
* ANN are more effective with tasks on unstructured data: text, audio, images, video, video-captioning, ...

---

# Artificial Neural Networks: Examples

* ANN encompasses many infrastructures: 
  1. [RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) for sequence data with short dependencies
  1. [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) for sequence data with short and long dependencies
  1. [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network) for images with 2D and 3D (+[RGB](https://en.wikipedia.org/wiki/RGB_color_model)) spatial dependencies
  1. [U-Net](https://en.wikipedia.org/wiki/U-Net) is an improved CNN
  1. [VAE](https://en.wikipedia.org/wiki/Variational_autoencoder) to compress representation of images, audio, ...
  1. [GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network) to generate new observations (e.g. faces, voices) from the training distribution
  1. [Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) builds "*attention*" to the "*important*" input data (e.g. lesser value of common stop words in text)
  1. [DRL](https://en.wikipedia.org/wiki/Deep_reinforcement_learning) trains an agent to take max-reward actions based on current state and past history (e.g. gaming, robotics)
  1. [GNN](https://en.wikipedia.org/wiki/Graph_neural_network) for graph-based data (e.g. social network, street maps, citation network)
  1. [RBM](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine) to learn the distribution of input for generative tasks
  1. [SOM](https://en.wikipedia.org/wiki/Self-organizing_map) for dimension reduction with maintaining the topological structure

---

# Artificial Neural Networks: Examples

* ANN encompasses many infrastructures: 
  1. <span style="background-color:aquamarine">[RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network) for sequence data with short dependencies</span> **28.11 - 04.12 week**
  1. <span style="background-color:aquamarine">[LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) for sequence data with short and long dependencies</span>
  1. <span style="background-color:bisque">[CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network) for images with 2D and 3D (+[RGB](https://en.wikipedia.org/wiki/RGB_color_model)) spatial dependencies</span> **05.12 - 11.12 week**
  1. [U-Net](https://en.wikipedia.org/wiki/U-Net) is an improved CNN
  1. <span style="background-color:lightpink">[VAE](https://en.wikipedia.org/wiki/Variational_autoencoder) to compress representation of images, audio, ...</span> **12.12 - 18.12 week**
  1. <span style="background-color:lightpink">[GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network) to generate new observations (e.g. faces, voices) from the training distribution</span>
  1. [Transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) builds "*attention*" to the "*important*" input data (e.g. lesser value of common stop words in text)
  1. [DRL](https://en.wikipedia.org/wiki/Deep_reinforcement_learning) trains an agent to take max-reward actions based on current state and past history (e.g. gaming, robotics)
  1. [GNN](https://en.wikipedia.org/wiki/Graph_neural_network) for graph-based data (e.g. social network, street maps, citation network)
  1. [RBM](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine) to learn the distribution of input for generative tasks
  1. [SOM](https://en.wikipedia.org/wiki/Self-organizing_map) for dimension reduction with maintaining the topological structure